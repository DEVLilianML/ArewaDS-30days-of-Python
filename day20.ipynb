{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# URL for Romeo and Juliet\n",
    "romeo_and_juliet_url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "\n",
    "resp = requests.get(romeo_and_juliet_url)\n",
    "print(f'Status code for the romeo_and_juliet_url: {resp.status_code}')\n",
    "\n",
    "# Get text content from the URL\n",
    "romeo_and_juliet_text = get_text_from_url(romeo_and_juliet_url)\n",
    "\n",
    "# Cleaning the text by removing non-alphanumeric characters\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', romeo_and_juliet_text, re.I)\n",
    "words = cleaned_text.split()\n",
    "\n",
    "word_counts = {}\n",
    "for word in words:\n",
    "    if word in word_counts:\n",
    "        word_counts[word] += 1\n",
    "    else:\n",
    "        word_counts[word] = 1\n",
    "\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words = sorted_words[:10]\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "response = requests.get(cats_api)\n",
    "cat_data = response.json()\n",
    "\n",
    "# Ensure the 'cat_data' variable is already defined from the first question\n",
    "\n",
    "# Dictionary to store country and breed frequencies\n",
    "country_breed_frequency = {}\n",
    "\n",
    "# Extract relevant information from the existing 'cat_data'\n",
    "for cat in cat_data:\n",
    "    # Create frequency table for country and breed\n",
    "    if 'origin' in cat and 'name' in cat:\n",
    "        origin = cat['origin']\n",
    "        breed = cat['name']\n",
    "        country_breed_key = f\"{origin} - {breed}\"\n",
    "\n",
    "        # Update frequency table\n",
    "        country_breed_frequency[country_breed_key] = country_breed_frequency.get(country_breed_key, 0) + 1\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFrequency Table - Country and Breed:\")\n",
    "for key, value in country_breed_frequency.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\"\"\"\n",
    "print('There is an issue with the url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the countries API and find\n",
    "#the 10 largest countries\n",
    "#the 10 most spoken languages\n",
    "#the total number of languages in the countries API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Countries API URL\n",
    "countries_api = 'https://restcountries.com/v3.1/all'\n",
    "\n",
    "# Fetch data from the Countries API\n",
    "response = requests.get(countries_api)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse JSON data\n",
    "    countries_data = response.json()\n",
    "\n",
    "    # Create a DataFrame from the JSON data\n",
    "    countries_df = pd.DataFrame(countries_data)\n",
    "\n",
    "    # Find the 10 largest countries\n",
    "    largest_countries = countries_df.nlargest(10, 'area')\n",
    "\n",
    "    # Find the 10 most spoken languages\n",
    "    languages_count = countries_df['languages'].explode().value_counts()\n",
    "    most_spoken_languages = languages_count.nlargest(10)\n",
    "\n",
    "    # Calculate the total number of languages\n",
    "    total_languages = languages_count.size\n",
    "    \n",
    "    # Print column names\n",
    "    print(\"Column Names:\", largest_countries.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCI is one of the most common places to get data sets for data science and machine learning. \n",
    "#Read the content of UCL (https://archive.ics.uci.edu/ml/datasets.php). Without additional libraries it will be difficult,\n",
    "# so you may try it with BeautifulSoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the UCI Machine Learning Repository\n",
    "uci_url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# Make a request to the URL\n",
    "response = requests.get(uci_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and print the text content of the page\n",
    "    print(soup.get_text())\n",
    "else:\n",
    "    print(f\"Failed to fetch content. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
